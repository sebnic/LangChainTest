from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from typing import Any, Dict
import sys


def invoke_llm_chain_with_error_handling(chain: Any, input_data: Dict[str, str], context: str = "LLM") -> str:
    """
    M√©thode centralis√©e pour g√©rer les erreurs lors des appels √† un LLM.
    
    Args:
        chain: La cha√Æne LangChain √† invoquer
        input_data: Les donn√©es d'entr√©e pour la cha√Æne
        context: Contexte de l'appel (pour le message d'erreur)
    
    Returns:
        str: Le r√©sultat de la cha√Æne ou un message d'erreur
    """
    try:
        resultat = chain.invoke(input_data)
        return resultat
    
    except ConnectionError as e:
        error_msg = f"‚ùå Erreur de connexion √† {context}: Impossible de se connecter au serveur Ollama."
        print(error_msg)
        print(f"   D√©tails: {str(e)}")
        print("   üí° V√©rifiez qu'Ollama est bien lanc√© avec: ollama serve")
        return None
    
    except TimeoutError as e:
        error_msg = f"‚ùå Timeout lors de l'appel √† {context}: Le serveur met trop de temps √† r√©pondre."
        print(error_msg)
        print(f"   D√©tails: {str(e)}")
        return None
    
    except ValueError as e:
        error_msg = f"‚ùå Erreur de valeur dans {context}: Donn√©es d'entr√©e invalides."
        print(error_msg)
        print(f"   D√©tails: {str(e)}")
        return None
    
    except Exception as e:
        error_msg = f"‚ùå Erreur inattendue lors de l'appel √† {context}."
        print(error_msg)
        print(f"   Type d'erreur: {type(e).__name__}")
        print(f"   D√©tails: {str(e)}")
        return None


# Cr√©ation du mod√®le Ollama
llm = ChatOllama(
    model="deepseek-r1:1.5b",
    temperature=0.7
)

# Parser pour extraire le texte de la r√©ponse
output_parser = StrOutputParser()

# PromptTemplate avec la contrainte d'expliquer en termes simples
prompt_template = PromptTemplate(
    input_variables=["concept"],
    template="""Tu es un excellent vulgarisateur scientifique. 
Ta mission est d'expliquer des concepts complexes de mani√®re SIMPLE et ACCESSIBLE √† tous.

Utilise des analogies de la vie quotidienne, √©vite le jargon technique, et reste clair.

Concept √† expliquer : {concept}

Explication simple :"""
)

# Cr√©ation de la cha√Æne LCEL : prompt | llm | parser
chain = prompt_template | llm | output_parser

# Ex√©cution de la cha√Æne avec gestion d'erreurs
print("=" * 60)
print("EXPLICATION EN TERMES SIMPLES")
print("=" * 60)
print()

resultat = invoke_llm_chain_with_error_handling(
    chain=chain,
    input_data={"concept": "l'ordinateur quantique"},
    context="Ollama (deepseek-r1:1.5b)"
)

if resultat:
    print(resultat)
    print()
    print("=" * 60)
else:
    print("\n‚ö†Ô∏è  L'ex√©cution a √©chou√©. Veuillez corriger les erreurs ci-dessus.")
    sys.exit(1)
